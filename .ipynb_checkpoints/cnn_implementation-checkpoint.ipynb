{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import keras_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5824 files belonging to 3 classes.\n",
      "Found 1458 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# importing the date prepared in data_prep.ipynb\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='train_test/train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    seed=1992,\n",
    "    shuffle=False)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='train_test/test',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    seed=1992,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing a data augmentation sequence to add more samples to the dataset\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    # scale and rotation\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2), \n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2), \n",
    "    # color\n",
    "    tf.keras.layers.RandomContrast(factor=0.2),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.2),\n",
    "    keras_cv.layers.RandomSaturation(factor=0.2),\n",
    "    keras_cv.layers.RandomHue(factor=0.2, value_range=(0,255)),\n",
    "    keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0,255)),\n",
    "    keras_cv.layers.ChannelShuffle(),\n",
    "    keras_cv.layers.RandomColorDegeneration(factor=0.2),\n",
    "    # noise\n",
    "    tf.keras.layers.GaussianNoise(0.1),\n",
    "    keras_cv.layers.RandomShear(x_factor=0.3,y_factor=0.3,),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model layers\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(xy, xy, 3)),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 23:51:58.489700: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [5824]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-12-17 23:51:58.489987: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [5824]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - ETA: 0s - loss: 2.6233 - accuracy: 0.3292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 23:54:44.073026: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [1458]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-12-17 23:54:44.073219: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [1458]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 236s 1s/step - loss: 2.6233 - accuracy: 0.3292 - val_loss: 1.9582 - val_accuracy: 0.3464\n",
      "Epoch 2/10\n",
      "182/182 [==============================] - 233s 1s/step - loss: 1.9532 - accuracy: 0.3159 - val_loss: 1.6381 - val_accuracy: 0.3368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule)\n\u001b[1;32m      8\u001b[0m model_cnn\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m     11\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m training_history \u001b[38;5;241m=\u001b[39m model_cnn\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mtest_ds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/training.py:1673\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1671\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1674\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/training.py:2428\u001b[0m, in \u001b[0;36mModel.reset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m original_pss_strategy\n\u001b[1;32m   2426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(all_outputs)\n\u001b[0;32m-> 2428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the state of all the metrics in the model.\u001b[39;00m\n\u001b[1;32m   2430\u001b[0m \n\u001b[1;32m   2431\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m \n\u001b[1;32m   2446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model_cnn.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "training_history = model_cnn.fit(train_ds, epochs=10, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "import pickle\n",
    "\n",
    "# Pickle the model\n",
    "with open(\"model_cnn.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model_cnn, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(test_ds, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "    model_cnn, \n",
    "    tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = probability_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### varify prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first image in the testset\n",
    "for images, labels in test_ds.take(1):\n",
    "    first_image = images[0]\n",
    "    first_label = labels[0]\n",
    "\n",
    "print(first_image.shape)\n",
    "print(first_label)\n",
    "plt.imshow(first_image.numpy().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see the prediction\n",
    "img_array = np.expand_dims(first_image, axis=0)\n",
    "probability_model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_prediction = model.predict(x_test)\n",
    "# result = confusion_matrix(y_test, y_prediction , normalize='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = len(training_history.history.get('loss',[]))\n",
    "\n",
    "# Draw Model Accuracy\n",
    "plt.figure(2,figsize=(6,4))\n",
    "plt.plot(range(epoch),history_cnn.history.get('accuracy'))\n",
    "#plt.plot(range(epoch),training_history.history.get('val_acc'))\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','validation'],loc=4)\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "# Draw Model Loss\n",
    "plt.figure(1,figsize=(6,4))\n",
    "plt.plot(range(epoch),history_cnn.history.get('loss'))\n",
    "#plt.plot(range(epoch),training_history.history.get('val_loss'))\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','validation'], loc=4)\n",
    "plt.style.use(['classic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**\n",
    "\n",
    "My goal in this project is to classify three artistic styles using neural networks. In this notebook, I've used CNN to achieve this task. The first step was using Keras to read the data from the directories. Loading the data in one batch caused my system to keep crashing. Then I build an augmentation sequence to extend the dataset by manipulating the samples. \n",
    "\n",
    "**CNN**\n",
    "\n",
    "I combined different convolutional, normalization, and regularization layers to increase the accuracy of the model. I've experienced multiple variations and I've achieved the most accuracy with this. I've modified the learning rate to speed up the convergence and I've used categorical cross-entropy to evaluate the hot encoding loss.\n",
    "\n",
    "**Results**\n",
    "\n",
    "At the beginning results of the model were unsatisfactory. The model achieved 50% accuracy which is only 17% better than random. That is mainly because my training data was around 1500 samples from each of the classes.\n",
    "\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "- The main challenge was reading the data without crashing my system. That's how I learned about approaches to reading data through Keras\n",
    "- Another challenge is the sample size. I believe that my sample size is relatively small and increasing it can lead to higher accuracy\n",
    "- similarity of styles is also a challenge since some arts have subtle differences. While working on this project I was wondering if I could achieve more accuracy with a higher sample size or with more distinguishable art styles\n",
    "\n",
    "**Future work**\n",
    "\n",
    "I would like to read more about the work in this space to learn about how other people did this. I believe that I can increase the accuracy even further by understanding the differences within these styles and then embedding them within the training layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
