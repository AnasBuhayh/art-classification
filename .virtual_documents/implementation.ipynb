








import numpy as np
import matplotlib.pyplot as plt
import pickle
import tensorflow as tf
import matplotlib.image as mpimg
import keras_cv


# importing the date prepared in data_prep.ipynb
train_ds = tf.keras.utils.image_dataset_from_directory(
    directory='train_test/train',
    labels='inferred',
    label_mode='categorical',
    batch_size=32,
    image_size=(256, 256),
    seed=1992,
    shuffle=False)

test_ds = tf.keras.utils.image_dataset_from_directory(
    directory='train_test/test',
    labels='inferred',
    label_mode='categorical',
    batch_size=32,
    image_size=(256, 256),
    seed=1992,
    shuffle=False)





xy = 256


# preparing a data augmentation sequence to add more samples to the dataset
data_augmentation = tf.keras.Sequential([
    # scale and rotation
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2), 
    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2), 
    # color
    tf.keras.layers.RandomContrast(factor=0.2),
    tf.keras.layers.RandomBrightness(factor=0.2),
    keras_cv.layers.RandomSaturation(factor=0.2),
    keras_cv.layers.RandomHue(factor=0.2, value_range=(0,255)),
    keras_cv.layers.RandomSharpness(factor=0.2, value_range=(0,255)),
    keras_cv.layers.ChannelShuffle(),
    keras_cv.layers.RandomColorDegeneration(factor=0.2),
    # # noise
    tf.keras.layers.GaussianNoise(0.1),
    keras_cv.layers.RandomShear(x_factor=0.3,y_factor=0.3,),

])


model_cnn = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(xy, xy, 3)),

    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.GlobalAveragePooling2D(),

    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2, activation='softmax')
])


# stops the training if the model stops improving for 5 epochs
early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(
    patience = 3, 
    restore_best_weights = True, 
    verbose = 1,
    monitor='val_loss')


# setting up learning rate for faster convergance
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)

# setting the opitmizer
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model_cnn.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=['accuracy'] 
)

# fitting the model
history_cnn = model_cnn.fit(train_ds, 
                                 epochs=5,
                                 validation_data=test_ds,
                                 callbacks = [early_stopping_callbacks])


# saving the model
import pickle

# Pickle the model
with open("model_cnn.pkl", "wb") as file:
    pickle.dump(model_cnn, file)





test_loss_cnn, test_acc_cnn = model_cnn.evaluate(test_ds, verbose=2)

print('\nTest accuracy:', test_acc_cnn)


probability_model = tf.keras.Sequential([
    model_cnn, 
    tf.keras.layers.Softmax()])

predictions = probability_model.predict(test_ds)





predictions[0]


test_ds.class_names


# print the first image in the testset
for images, labels in test_ds.take(1):
    first_image = images[0]
    first_label = labels[0]

print(first_image.shape)
print(first_label)
plt.imshow(first_image.numpy().astype(int))


# now let's see the prediction
img_array = np.expand_dims(first_image, axis=0)
probability_model.predict(img_array)


test_ds.class_names





# from sklearn.metrics import confusion_matrix

# y_prediction = model.predict(x_test)
# result = confusion_matrix(y_test, y_prediction , normalize='pred')





epoch = len(history_cnn.history.get('loss',[]))

# Draw Model Accuracy
plt.figure(2,figsize=(6,4))
plt.plot(range(epoch),history_cnn.history.get('accuracy'))
#plt.plot(range(epoch),training_history.history.get('val_acc'))
plt.xlabel('# Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.grid(True)
plt.legend(['train','validation'],loc=4)
plt.style.use(['classic'])

# Draw Model Loss
plt.figure(1,figsize=(6,4))
plt.plot(range(epoch),history_cnn.history.get('loss'))
#plt.plot(range(epoch),training_history.history.get('val_loss'))
plt.xlabel('# Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.grid(True)
plt.legend(['train','validation'], loc=4)
plt.style.use(['classic'])











# defining the layers
model_nn = tf.keras.Sequential([
    data_augmentation,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(2) 
])


# stops the training if the model stops improving for 5 epochs
early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(
    patience = 3, 
    restore_best_weights = True, 
    verbose = 1,
    monitor='val_loss')


# setting up learning rate for faster convergance
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True
)

# setting the opitmizer
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
model_nn.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'] 
)

# fitting the model
history_nn = model_nn.fit(train_ds, 
                                 epochs=5,
                                 validation_data=test_ds,
                                 callbacks = [early_stopping_callbacks])


import pickle

# Pickle the model
with open("model_nn.pkl", "wb") as file:
    pickle.dump(model_nn, file)



test_loss_nn, test_acc_nn = model_nn.evaluate(test_ds, verbose=2)

print('\nTest accuracy:', test_acc_nn)


probability_model = tf.keras.Sequential([
    model_nn, 
    tf.keras.layers.Softmax()])

predictions = probability_model.predict(test_ds)





predictions[0]


test_ds.class_names


# print the first image in the testset
for images, labels in test_ds.take(1):
    first_image = images[0]
    first_label = labels[0]

print(first_image.shape)
print(first_label)
plt.imshow(first_image.numpy().astype(int))


# now let's see the prediction
img_array = np.expand_dims(first_image, axis=0)
probability_model.predict(img_array)


test_ds.class_names








epoch = len(history_nn.history.get('loss',[]))

# Draw Model Accuracy
plt.figure(2,figsize=(6,4))
plt.plot(range(epoch),history_nn.history.get('accuracy'))
#plt.plot(range(epoch),training_history.history.get('val_acc'))
plt.xlabel('# Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.grid(True)
plt.legend(['train','validation'],loc=4)
plt.style.use(['classic'])

# Draw Model Loss
plt.figure(1,figsize=(6,4))
plt.plot(range(epoch),history_nn.history.get('loss'))
#plt.plot(range(epoch),training_history.history.get('val_loss'))
plt.xlabel('# Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.grid(True)
plt.legend(['train','validation'], loc=4)
plt.style.use(['classic'])















