


import random
import glob
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg # for loading in images

%matplotlib inline
%env PYDEVD_DISABLE_FILE_VALIDATION=1





# reading styles
import os

# get the styles names
styles = os.listdir('archive')

# styles_lables = list()

# add target label to the styles
# for index, style in enumerate(styles):
#     styles_lables.append((style, index))

print(styles)
# print(styles_lables)


# check how many files per style
for style in styles:
    os.listdir((f'archive/{style}/{style}'))
    print(style, len(os.listdir((f'archive/{style}/{style}'))))





xy = 256


import cv2 

# This function should take in an RGB image and return a new, standardized version
def standardize_input(image):
    image_crop = np.copy(image)
    row_crop = 7
    col_crop = 8
    image_crop = image[row_crop:-row_crop, col_crop:-col_crop, :]
    ## TODO: Resize image and pre-process so that all "standard" images are the same size
    standard_im = cv2.resize(image_crop, (xy, xy))
    return standard_im


# test image
test_img = glob.glob(f'archive/Western_Medieval/Western_Medieval/9223372032559869461.jpg')[0]
test_img = mpimg.imread(test_img)
print(test_img.shape)
plt.imshow(test_img)





#standarize the image
std_img = standardize_input(test_img)
print(std_img.shape)
plt.imshow(std_img)


# reading samples and adding lables
def read_files(style):
    ims_list = list()

    for file in glob.glob(f'archive/{style[0]}/{style[0]}/*'):
        # Read in the image
        im = mpimg.imread(file)
        
        if len(im.shape) < 3: #skip black and white images
                continue
        
        # I need to add a cmyk conversion in here
        if im.shape[2] != 3: #make sure to keep only rgb channels
                continue
    
        try:
            
            # standarize(im) 28x28
            std_im = standardize_input(im)
            
            # normalize(im) im/255
            norm_std_im = std_im/255
            
            # Append the image to the green image list

            ims_list.append((norm_std_im,style[1]))
        
        except ValueError as e:
            print(f"Caught an exception: {e}")
            print(file)

    return ims_list


styles


selected_styles = [('Japanese_Art', 0), ('Rococo', 1), ('Primitivism', 2)]


print("reading -->", selected_styles[0][0])
style1 = read_files(selected_styles[0])
print("No. images -->", len(style1), "\n")
print(" ======= ")

print("reading -->", selected_styles[1][0])
style2 = read_files(selected_styles[1])
print("No. images -->", len(style2), "\n")
print(" ======= ")

print("reading -->", selected_styles[2][0])
style3 = read_files(selected_styles[2])
print("No. images -->", len(style3), "\n")


# import pickle

# with open("style1", 'wb') as pfile:
#     pickle.dump(style1, pfile, protocol=pickle.HIGHEST_PROTOCOL)

# with open("style2", 'wb') as pfile:
#     pickle.dump(style2, pfile, protocol=pickle.HIGHEST_PROTOCOL)

# with open("style3", 'wb') as pfile:
#     pickle.dump(style3, pfile, protocol=pickle.HIGHEST_PROTOCOL)


# import pickle

# with open("style1", 'rb') as pfile:
#     style1 = pickle.load(pfile)

# with open("style2", 'rb') as pfile:
#     style2 = pickle.load(pfile)

# with open("style3", 'rb') as pfile:
#     style3 = pickle.load(pfile)



ims_list = style1 + style2 + style3


#check the images count
len(ims_list)


# randomize the images
import random
random.seed(31101992)

random.shuffle(ims_list)


# split train and test labels
X = [img[0] for img in ims_list]
y = [label[1] for label in ims_list]


expected_shape = (xy, xy, 3)

for image in X:
    if image.shape != expected_shape:
        print(f"Invalid shape found: {image.shape}. Expected shape: {expected_shape}")
        # You may choose to break the loop or raise an exception if an invalid shape is found

# If the loop completes without printing, all shapes are as expected
print("All images have the expected shape.")


# split the data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=31, stratify=y
)


# clearning up the cashe
del X, y, ims_list, style1, style2, style3





import tensorflow as tf


# convert to np.array in batches because the codes keeps breaking
batch_size = 64 

X_batches = []

for i in range(0, len(X_train), batch_size):
    X_batch = np.array(X_train[i:i + batch_size])

    X_batches.append(X_batch)

# Concatenate batches to get the final numpy arrays
X_train = np.concatenate(X_batches, axis=0)
y_train = np.array(y_train)


# convert the data to np.array for tensorflow
batch_size = 64 

X_batches = []
y_batches = []

for i in range(0, len(X_test), batch_size):
    X_batch = np.array(X_test[i:i + batch_size])

    X_batches.append(X_batch)

# Concatenate batches to get the final numpy arrays
X_test = np.concatenate(X_batches, axis=0)
y_test= np.array(y_test)


data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),
])


model_nn = tf.keras.Sequential([
    data_augmentation,
    tf.keras.layers.Flatten(input_shape=(xy, xy, 3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3) 
])


initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True
)

model_nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


early_stopping = tf.keras.callbacks.EarlyStopping(
    patience=10, restore_best_weights=True
)

history_nn = model_nn.fit(X_train, y_train, epochs=10, callbacks=[early_stopping])


test_loss_nn, test_acc_nn = model_nn.evaluate(X_test,  y_test, verbose=2)

print('\nTest accuracy:', test_acc_nn)


import pickle

# Pickle the model
with open("model_nn.pkl", "wb") as file:
    pickle.dump(model_nn, file)






model_cnn = tf.keras.Sequential([
    data_augmentation,
    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape=(xy, xy, 3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),

    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

model_cnn.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]
)

history_cnn =  model_cnn.fit(X_train, y_train, epochs=10, callbacks=[early_stopping])


test_loss_cnn, test_acc_cnn = model_cnn.evaluate(X_test,  y_test, verbose=2)

print('\nTest accuracy:', test_acc_cnn)


import pickle

# Pickle the model
with open("model_cnn.pkl", "wb") as file:
    pickle.dump(model_cnn, file)



